### 2020-10-27
## 오늘 반드시 기억할 것.
- sklearn 패키지 이용 구조를 기억하자.
1. 패키지 임포트
2. 데이터셋 불러오기
3. 모델생성
4. 작업
5. 평가

======================================================================================================
# Chap09_Ensemble_step02_Cross_validation (교차분할검정)
 - 훈련셋과 검정셋을 서로 교차하여 검정하는 방법(N등분)

1. 패키지 임포트
from sklearn.dataset import load_digits # dataset
from sklearn.model_selection import cross_validate # 교차검정
from sklearn.ensemble import RandomForestClassifier # model

2. dataset load
X, y = load_digits(return_X_y=True)
print(X)
print(X.shape)
print(y)

3. 모델 생성
obj = RandomForestClassifier()
model = obj.fit(X=X, y=y)

score = model.score(X=X, y=y)
print(score)

4. 교차 검정
score = cross_validate(model, X, y, cv=5)
print(score['test_score'])

5. 모델 평가 - 산술평균
print(score['test_score'].mean()) 

=================================================================================================

# Chap09_Ensemble_step03_RF parameter 
'''
1. RandomForest Hyper parameter
2. RandomForest + GridSearch
''''

# 1. 데이터셋 로드
wine = load_wine()
print(wine.feature_names) # x를 특징변수라고도 함

X, y = load_wine(return_X_y=True)
print(X.shape) # (178, 13)
print(y) # 0/1/2

# 2. RM model
rfc = RandomForestClassifier() # default params

'''
<Hyper Parameter> 
1. criterion='gini' : 노드 불순도 - 중요변수 선정기준
2. max_depth : 트리의 깊이(클 수록 서능이 좋아짐, 오버피팅 문제)
3. max_features='auto' : 최대 사용할 x변수 개수
4. n_estimators : 결정 트리 개수(default=10), 많을 수록 성능이 좋아짐
5. n_jobs=None : cpu 사용 수
6. min_samples_leaf : leaf node를 만드는데 필요한 최소한의 sample 수
7. min_samples_split : 내부 노드를 분할하는 데 필요한 최소 sample 수
'''

model = rfc.fit(X=X, y=y) # full dataset 
print(model)

# 3. Grid Search model 
parmas = {'n_estimators':[100,150,200,300],
          'max_depth':[None,3, 5, 7],
          'min_samples_leaf':[3,5,7,9],
          'min_samples_split':[2,3,4,5],
          'max_features' : ['auto','sqrt']} # dict

# 5겹 교차검정, 평가방법 : 분류정확도, 가용 cpu 사용 
grid = GridSearchCV(estimator=model, param_grid=parmas, 
             scoring='accuracy', cv=5, n_jobs=-1)
print(grid)

##########################################
GridSearchCV 를 통해서 각 변수 안에 있는 것들의 조합중에서 가장 fit한것을 찾아냄
#########################################

grid_model = grid.fit(X, y)

# 4. Best Hyper parameter
print('best score =', grid_model.best_score_)
# best score = 0.935468895078923

print('best parameters =', grid_model.best_params_)

'''
가장 best 한 parameter
best parameters = {'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 200}
'''


=================================================================================================

# Chap09_Ensemble_step04_XGBoost_test
아나콘다 프롬포트에서 pip install xgboost 필요함

from xgboost import XGBClassifier, XGBRegressor # 분류Tree, 회귀Tree
from xgboost import plot_importance # 중요변수 시각화 
from sklearn.datasets import make_blobs # cluster=3 dataset 
from sklearn.model_selection import train_test_split # split
# model 평가 
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt # data 분포(산점도)

# 1. dataset 생성 
X, y = make_blobs(n_samples=2000, n_features=4, centers=3, 
             cluster_std=1.5, random_state=123)
'''
n_samples : 표본 데이터의 수 
n_features : x변수 개수 
centers : y변수 범주의 수(=cluster) 
cluster_std : 클러스터 표준편차 
'''
print(X.shape) # (2000, 4)
print(y) # [1 1 0 ... 0 0 2]

# data 분포 시각화 
plt.scatter(x=X[:, 0], y=X[:, 1], c=y, marker='o')
plt.show()


# 2. training/test split 
x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size = 0.3)


# 3. XGB model 
obj = XGBClassifier() # default params
model = obj.fit(X=x_train, y=y_train)


# 4. model 평가 
y_pred = model.predict(x_test)

acc = accuracy_score(y_test, y_pred)
print('accuracy =', acc) # accuracy = 0.9733333333333334

report = classification_report(y_test, y_pred)
print(report)
'''
              precision    recall  f1-score   support

           0       0.95      0.97      0.96       207
           1       1.00      1.00      1.00       204
           2       0.97      0.95      0.96       189

    accuracy                           0.97       600
   macro avg       0.97      0.97      0.97       600
weighted avg       0.97      0.97      0.97       600
'''

# 5. 중요변수 시각화 
fscore = model.get_booster().get_fscore() 
print('fscore =', fscore)
# fscore = {'f3': 308, 'f2': 394, 'f0': 315, 'f1': 297}

plot_importance(booster=model)
plt.show()

=========================================================================================
# Chap09_Ensemble_step05_GBRegressor
 - XGBRegressor : 회귀tree model 생성 

from xgboost import XGBRegressor # 회귀Tree model
from xgboost import plot_importance # 중요변수 시각화 

from sklearn.datasets import load_boston # dataset 
from sklearn.model_selection import train_test_split # split
# model 평가 
from sklearn.metrics import mean_squared_error, r2_score
import matplot.pyplot as plt 

# 1. dataset load 
boston = load_boston()
x_names = boston.feature_names
print(x_names) 
# ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO' 'B' 'LSTAT']

len(x_names) # 13

X = boston.data
y = boston.target

# 2. train_test_split
x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size = 0.3)


# 3. model 
obj = XGBRegressor() # default params
model = obj.fit(X=x_train, y=y_train)


# 4. 중요변수 시각화 
fscore = model.get_booster().get_fscore()
print('fscore :', fscore)
# fscore : {'f12': 323, 'f5': 428, 'f7': 305, 'f0': 714, 'f11': 298, 'f4': 210, 'f10': 77, 'f6': 311, 'f8': 36, 'f2': 109, 'f9': 77, 'f3': 14, 'f1': 67}


plot_importance(fscore)
# f0[범죄율] -> f5[대기오염] -> f12[하위계층]

# 5. model 평가
y_pred = model.predict(x_test) 

mse = mean_squared_error(y_test, y_pred)
print('MSE =', mse) # MSE = 8.867044756571998
score = r2_score(y_test, y_pred)
print('r2 score =', score) # r2 score = 0.8990757949048808

# pred vs real value
fig = plt.figure()
chart = fig.add_subplot()

chart.plot(y_pred[:50], marker='o', color='blue', linestyle='-', label='y predicted')
chart.plot(y_test[:50], marker='+', color='red', linestyle='--', label='y real value')
chart.set_title('prediction vs real value')
plt.xlabel('index')
plt.ylabel('predicted')
plt.legend(loc ='best')
plt.show()


# 6. model save/load
import pickle

path="C:/ITWILL/4_Python-II/workspace/chap09_Ensamble/"
fileName = 'xgb_model.pkl'

# model save
file = open(path+fileName, mode='wb')
pickle.dump(model, file)

# model load
file2 = open(path+fileName, mode='rb')
load_model = pickle.load(file2)


y_pred = load_model.predict(x_test)
print(y_pred)

score = r2_score(y_test, y_pred)
print('r2 score =', score)
# r2 score = 0.8990757949048808
